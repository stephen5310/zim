Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2023-01-17T23:42:18+01:00

====== 8. Networking ======
Created mardi 17 janvier 2023

===== Prerequisites =====

==== Network NS ====

* Used to create isolated networks on a host
* Create with : **ip netns add <ns_name>**
* List ns with :  **ip netns**
* Execute command in ns : **ip netns exec <ns_name> <command>** OR **ip -n <ns_name> <command>**
* Link two ns with veth link : 
	* Create link : **ip link add <veth-1> type veth peer name <veth-2>**
	* Attach : **ip link set <veth-1> netns <ns-1>** AND **ip link set <veth-2> netns <ns-2>**
	* Add IPs : **ip -n <ns-1> addr add <ip-1>/<mask> dev <veth-1>** AND **ip -n <ns-2> addr add <ip-2>/<mask>**
	* Activate interfaces : **ip -n <ns-1> link set <veth-1> up** AND **ip -n <ns-2> link set <veth-2> up**
	* Ping each other : **ip netns exec <ns-1> ping <ip-2>**
	* Get arp table : **ip netns exec <ns-1> arp**
	* Delete the link on only one end : **ip -n <ns-1> link del <veth-1>**
* Use virtual switch solution to allow many NS to communicate with each other : Linux Bridge, Open vSwitch
	* Create linux brighe vswitch : **ip link add <br> type bridge**
	* Bring up : **ip link set dev <br> up**
	* Add ip subnet: **ip addr add <br-ip>/<mask> dev <br>**
	* Connect all ns to bridge :
		* Create link for ns :  **ip link add <veth> type veth peer name <veth-br>**
		* Attach link to ns : **ip link set <veth> netns <ns>**
		* Connect link to bridge : **ip link set <veth-br> master <br>**
		* Set ip address : **ip -n <ns> addr add <ip>/<mask> dev <veth>**
		* Bring up : **ip -n <ns> link set <veth> up**
* Add routes to ns to route packets
* Add iptables rules for NAT

==== Docker Networking ====

* Networking options with **--network** :
	* none : Container not attached to any network, cannot be reached from other container nor from the outside
	* host : Use host interface and port, no port mapping nor NAT
	* bridge : Internal private network is created (default 172.17.0.0/16)
		* A bridge docker0 (default ip 172.17.0.1) is created during installation and a docker network called bridge is linked to it
		* A default ns is also created with a hash like name
		* When creating a container docker :
			* Creates a ns for it
			* Creates a veth link and attach an interface to the container ns and the other to the bridge
			* Interfaces pairs can be identified by odd and even numbers (1-2, 3-4, ..)
			* Add ip addresses and bring interfaces up
			* Use option -p for port mapping to make service available from outside of the host : **docker run -p <host_port>:<container_port> <image>**
			* Add iptables rule for port mapping

==== Container Network Interface ====

* Standard defined for programs development to allow them to solve networking challenges in a container runtime environment
* Programs are referred to as plugins
* Functionnalities required by Container Runtimes :
	* Create netns
	* Identify the network to attach the container to
	* Invoke Network Plugin when container is ADDed
	* Invoke Network Plugin when container is DELeted
	* JSON format of the network conf
* Functionnalities required by Plugin :
	* Support cli args ADD/DEL/CHECK
	* Support params container_id, netns, ...
	* Manage IPaddr assignment to PODs
	* Return results in a specific format
* Supported plugins are : bridge, vlan, ipvlan, macvlan, windows, host-local, DHCP, weave, flannel, cilium, NSX, calico, infoblox
* Docker has its own model called CNM (Model)
* To use a bridge with docker like k8s does it :
	* Create container with none network
	* Invoke the bridge manually : **bridge add <container_hash> <path_to_netns>**

===== Networking cluster nodes =====

* Master and workers nodes
* Each node must have at least one netinf connected to the same network
* Hosts must have unique hostnames set and MAC
* Some ports must be opened depending on the node type : https://kubernetes.io/docs/reference/networking/ports-and-protocols/

===== Pod Networking =====

* No built-in solution for k8s
* Expectations :
	* Every pod should have an IPaddr
	* Every pod should be able to communicate with pods on the same node
	* Every pod should be able to communicate with pods on other nodes without NAT
* Specify cni conf and bin to kubelet with options : **--network-plugin=cni --cni-conf-dir=<conf_path> --cni-bin-dir=<bin_path>**
* Kubelet chooses the conf file in alphabetical order
* The bin dir path contains the cnis that can be put in the conf file

==== Weave networking solution ====
* Deploys an agent on each node to gather infos about the pods
* Agents share infos about the nodes, networks and pods they discover
* Each agent stores the topology of the entire setup
* Creates its own bridge on the nodes called weave
* Adds necessary routes to pods to allow them to reach the agent
* Packets on seperate node are encapsulated by Weave agent and transmitted to the agent on the destination node for decapsulation and forwarding
* Weave can be deployed on each node as 
	* daemon : install as service
	* pod : **kubectl apply -f [[https://github.com/weaveworks/weave/releases/download/<version>/weave-daemonset-k8s.yaml]]**
	* Remember to install the same version as kubectl
* IPAM :
	* Default network for DHCP is 10.32.0.0/12
	* The peer agent split the range equally to assign each portion to each node

==== Service Networking ====

* Pods usually communicate using Services
* Services are cluster wide and managed by kube-proxy agent on each node
* They are virtual : no link creation etc
* IP is assigned in a range pre-defined in kube-api-server (default 10.0.0.0/24) : **kube-api-server --service-cluster-ip-range <network>**
* **!!! pods and services IP ranges should not overlap !!!!**
* Only forwarding rules are created on each node with userspace, ipvs or iptables (default)
* List rules with : **iptables -L -t nat | grep <service>**
* Use **--proxy-mode** option to set it
* Types of services :
	* ClusterIP : accessible only by pods cluster wide
	* NodePort : accessible by pods cluster wide and also from outside of the cluster on a port on all nodes in the cluser
* Consult **kube-proxy.log** to see logs

===== DNS =====

* When deploying k8s with kubeadm it comes with a kube-dns service
* When deploying the hard way one has to configure DNS by himself
* Record is created for service upon creation mapping service name with its IPaddr
* To access service from another ns just add ns as domain : **service.ns**
* All services are in subdomain **svc**
* Top domain level domain is by default **.cluster.local**
* Services fqdn are like : **service.ns.svc.cluster.local**
* Pods records automatic creation must be activated
* Pods fqdn are formed by replacing dots with dashes in IPaddr like : **10-244-5-1.ns.pod.cluster.local**
* DNS for the service is automatically created and address is configured in [[/etc/resolv.conf]] in containers for resolution by kubelet
* kube-dns is deployed by default in the kube-system ns with a replicaset of 2 pods inside a deployment
* Conf is at **[[/etc/coredns/CoreFile]]** with a number of plugins configured which are responsible of error management, health checks, k8s cluster connection, ...
* The **kubernetes** pluging conf options are :
	* top-level domain name
	* pods : insecure (to activate pods record automatic creation)
	* proxy : to specify DNS to contact in case of resolution failure
* Use a configmap to ease config modification
* A pod is only accessible with fqdn

===== Ingress =====

https://kubernetes.io/docs/concepts/services-networking/ingress
* Service of type LoadBalancer are available on Public clouds to automatically create a load balancer to proxy requests to app
* Ingress helps users access an app using a single externally accessible url configured by admin to route traffic to different services within the cluster based on the url path and also implement ssl security
* It is a layer 7 lb built into the k8s cluster that can be configured using native k8s primitives
* Ingress services must be exposed before being accessible : NodePort, LoadBalancer are supported
* Ingress uses a proxy solution as Ingress Controller to be deployed on the cluster : ngnix, haproxy, traefik, istio, contour, GCP HTTP LB
* Ingress manages the proxy configurations with Ingress Resources created using manifest of kind 
* Nginx and GCP Http LB are part of k8s project
* Deploy nginx ingress controller using a deployment with image nginx-ingress-controller :
	* Args will be :
		* [[/nginx-ingress-controller]] --configmap=$(POD_NAMESPACE)/nginx-configuration
		* The configmap can be empty but will be useful to change conf options
	* Env must have variables for pod name and pod namespace
	* Specifiy ports 80 and 443
	* Create a NodePort service to expose nginx
	* Create a ServiceAccount to give the right to access all objects needed
* An Ingress Resource is a set of rules and configuration options applied on the ingress controller
	* Create with a manifest of kind Ingress
	* Specify backend with **.spec.backend.service : name, port**
	* List ingress resources : **kubectl get ingress**
	* Rules are defined at the top for domain names
	* Each rules has a set of paths to route traffic based on url
	* Specify for only one domain rules with **.spec.rules[*] :**
		* Specify paths with .**http(s).paths[*].path**
		* Associate backend to path with **.http(s).paths[*].backend**
	* View details with : **kubectl describe ingress ingress-wear-watch**
	* There is a default rule to route traffic to a default backend **default-http-backend:80**, such backend must be defined
	* Rules for multiple domains use **.spec.rules[*].host** to specify hostname
	* Imperative resource creation : **kubectl create ingress <ingress-name> --rule="host/path=service:port"**

==== Deploy Nginx Ingress Controller ====
* Create ns : **kubectl create namespace <ns>**
* Create configmap : **kubectl create configmap <cfgmap> -n <ns>**
* Create two ServiceAccounts :
	* **kubectl create serviceaccount <sac1> -n <ns>**
	* **kubectl create serviceaccount <sac2> -n <ns>**
* Create Roles and RoleBindings for ServiceAccounts : 
* Create ClusterRoles and ClusterRoleBindings for ServiceAccounts : https://github.com/nginxinc/kubernetes-ingress/blob/main/deployments/rbac/rbac.yaml
* Deploy controller, modify file before : https://github.com/nginxinc/kubernetes-ingress/blob/main/deployments/deployment/nginx-ingress.yaml
